{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow import keras\n",
    "\n",
    "from spektral.data import Dataset\n",
    "from spektral.data import Graph\n",
    "from spektral.transforms import GCNFilter\n",
    "from spektral.data.loaders import SingleLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"k_pneumoniae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"../models/model_plasgraph_\" + species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Networkx_to_Spektral(Dataset):\n",
    "    def __init__(self, nx_graph, **kwargs):\n",
    "        self.nx_graph = nx_graph\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "\n",
    "        x = np.array(\n",
    "            [self.nx_graph.nodes[node_name][\"x\"] for node_name in self.nx_graph.nodes]\n",
    "        )\n",
    "\n",
    "        y = np.array(\n",
    "            [self.nx_graph.nodes[node_name][\"y\"] for node_name in self.nx_graph.nodes]\n",
    "        )\n",
    "\n",
    "        a = nx.adjacency_matrix(self.nx_graph)\n",
    "        a.setdiag(0)\n",
    "        a.eliminate_zeros()\n",
    "\n",
    "        # We must return a list of Graph objects\n",
    "        return [Graph(x=x.astype(float), a=a.astype(float), y=y.astype(float))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pentamer distributions\n",
    "\n",
    "kmer_length = 5\n",
    "\n",
    "k_mers = [\"\".join(x) for x in itertools.product(\"ACGT\", repeat=kmer_length)]\n",
    "\n",
    "\n",
    "fwd_kmers = []\n",
    "rev_kmers = []\n",
    "\n",
    "for k_mer in k_mers:\n",
    "    if not ((k_mer in fwd_kmers) or (k_mer in rev_kmers)):\n",
    "        fwd_kmers.append(k_mer)\n",
    "        rev_kmers.append(str(Seq(k_mer).reverse_complement()))\n",
    "\n",
    "\n",
    "def get_kmer_distribution(\n",
    "    sequence, k_mers=k_mers, fwd_kmers=fwd_kmers, kmer_length=5, scale=False\n",
    "):\n",
    "    if len(sequence) < 5:\n",
    "        return [0] * int(4**kmer_length / 2)\n",
    "    dict_kmer_count = {}\n",
    "\n",
    "    for k_mer in k_mers:\n",
    "        dict_kmer_count[k_mer] = 0\n",
    "\n",
    "    for i in range(len(sequence) + 1 - kmer_length):\n",
    "        kmer = sequence[i : i + kmer_length]\n",
    "        try:\n",
    "            dict_kmer_count[kmer] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    k_mer_counts = [\n",
    "        dict_kmer_count[k_mer] + dict_kmer_count[str(Seq(k_mer).reverse_complement())]\n",
    "        for k_mer in fwd_kmers\n",
    "    ]\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        k_mer_counts = scaler.fit_transform(np.array(k_mer_counts).reshape(-1, 1))\n",
    "        k_mer_counts = list(k_mer_counts.flatten())\n",
    "\n",
    "    return k_mer_counts\n",
    "\n",
    "\n",
    "# extract GC content\n",
    "def get_gc_content(seq):\n",
    "    number_gc = 0\n",
    "    number_acgt = 0\n",
    "    for base in seq.lower():\n",
    "        if base in \"gc\":\n",
    "            number_gc += 1\n",
    "        if base in \"acgt\":\n",
    "            number_acgt += 1\n",
    "    try:\n",
    "        gc_content = round(number_gc / number_acgt, 4)\n",
    "    except ZeroDivisionError:\n",
    "        gc_content = 0.5\n",
    "    return gc_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_isolates = [\n",
    "    \"kpne-SAMN21366037\",\n",
    "    \"kpne-SAMN21366038\",\n",
    "    \"kpne-SAMN21366043\",\n",
    "    \"kpne-SAMN21366050\",\n",
    "    \"kpne-SAMN21366063\",\n",
    "    \"kpne-SAMN21366069\",\n",
    "    \"kpne-SAMN21366070\",\n",
    "    \"kpne-SAMN21366091\",\n",
    "    \"kpne-SAMN21366094\",\n",
    "    \"kpne-SAMN21366095\",\n",
    "]\n",
    "\n",
    "graph_files = [\"../data/short_read_assembly_graph_files/\" + species + \"\\\\\" + isolate + \".gfa\" for isolate in held_out_isolates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kpne-SAMN21366037'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_files[0].split(\"\\\\\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\janik\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "\n",
    "for graph_file in graph_files:\n",
    "\n",
    "    isolate = graph_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "\n",
    "    dict_contig_length = {}\n",
    "    dict_contig_length_normalized = {}\n",
    "    tuple_node1_node2 = []\n",
    "    dict_contig_gc = {}\n",
    "    dict_contig_kmer = {}\n",
    "    dict_contig_coverage = {}\n",
    "    dict_contig_label = {}\n",
    "\n",
    "    dict_contig_kmer_euclidean_distance = {}\n",
    "    dict_contig_num_edges = {}\n",
    "\n",
    "    file_ = open(graph_file, \"r\")\n",
    "    lines = file_.readlines()\n",
    "    file_.close()\n",
    "\n",
    "    # get gc of whole seq\n",
    "\n",
    "    whole_seq = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            whole_seq += line.strip().split()[2]\n",
    "\n",
    "    gc_of_whole_seq = get_gc_content(whole_seq)\n",
    "\n",
    "    # get contig lengths and max length\n",
    "\n",
    "    max_contig_length = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_length[int(line.split()[1])] = len(\n",
    "                line.split()[2]\n",
    "            )\n",
    "            if len(line.split()[2]) > max_contig_length:\n",
    "                max_contig_length = len(line.split()[2])\n",
    "\n",
    "    # get normalized contig lengths and max length\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_length_normalized[\n",
    "                int(line.split()[1])\n",
    "            ] = (len(line.split()[2]) / max_contig_length)\n",
    "\n",
    "\n",
    "    # get graph edges\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"L\":\n",
    "            tuple_node1_node2.append((int(line.split()[1]), int(line.split()[3])))\n",
    "\n",
    "    # get gc content\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_gc[int(line.split()[1])] = (\n",
    "                get_gc_content(line.split()[2]) - gc_of_whole_seq\n",
    "            )\n",
    "\n",
    "    # get pentamer distributions\n",
    "    #kmer_length = 5\n",
    "\n",
    "    #k_mers = [\"\".join(x) for x in itertools.product(\"ACGT\", repeat=kmer_length)]\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_kmer[int(line.split()[1])] = get_kmer_distribution(\n",
    "                line.split()[2], k_mers=k_mers, scale=True\n",
    "            )\n",
    "\n",
    "    # get euclidian of pentamer distribution for each node\n",
    "\n",
    "    kmer_length = 5\n",
    "\n",
    "    k_mers = [\"\".join(x) for x in itertools.product(\"ACGT\", repeat=kmer_length)]\n",
    "\n",
    "    # generate dict with all contigs of current isolate and their pentamer distribution\n",
    "    dict_contig_kmer_current_isolate = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_kmer_current_isolate[\n",
    "                int(line.split()[1])\n",
    "            ] = get_kmer_distribution(line.split()[2], k_mers=k_mers)\n",
    "\n",
    "    # calculate total pentamer distribution and scale between 0 and 1\n",
    "    all_kmer_counts = [\n",
    "        sum(x) for x in zip(*list(dict_contig_kmer_current_isolate.values()))\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    all_kmer_counts = scaler.fit_transform(np.array(all_kmer_counts).reshape(-1, 1))\n",
    "    all_kmer_counts = list(all_kmer_counts.flatten())\n",
    "\n",
    "    # get euclidean distance for each contig and add to dict\n",
    "    for contig in dict_contig_kmer_current_isolate:\n",
    "        kmer_distribution = np.array(dict_contig_kmer_current_isolate[contig])\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_kmer_distribution = scaler.fit_transform(\n",
    "            np.array(kmer_distribution).reshape(-1, 1)\n",
    "        )\n",
    "        scaled_kmer_distribution = list(scaled_kmer_distribution.flatten())\n",
    "        dict_contig_kmer_euclidean_distance[contig] = np.linalg.norm(\n",
    "            np.array(all_kmer_counts) - np.array(scaled_kmer_distribution)\n",
    "        )\n",
    "\n",
    "    # get coverage\n",
    "\n",
    "    for line in lines:\n",
    "        if line.split()[0] == \"S\":\n",
    "            dict_contig_coverage[int(line.split()[1])] = round(\n",
    "                float(line.strip().split(\":\")[-1]), 2\n",
    "            )\n",
    "\n",
    "    # generate networkx graph\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for tpl in tuple_node1_node2:\n",
    "        G.add_edge(tpl[0], tpl[1])\n",
    "\n",
    "    # get number of edges per contig_\n",
    "\n",
    "    for contig_ in G.nodes:\n",
    "        dict_contig_num_edges[contig_] = len(list(G.neighbors(contig_)))\n",
    "\n",
    "    # make feature dict\n",
    "\n",
    "    dict_contig_list_coverage_gc_kmer = {}\n",
    "\n",
    "    for contig_ in G.nodes:\n",
    "        dict_contig_list_coverage_gc_kmer[contig_] = [\n",
    "            dict_contig_coverage[contig_],\n",
    "            dict_contig_gc[contig_],\n",
    "            dict_contig_kmer_euclidean_distance[contig_],\n",
    "            dict_contig_num_edges[contig_],\n",
    "            dict_contig_length_normalized[contig_]\n",
    "        ]  # + dict_contig_kmer[contig_]\n",
    "\n",
    "    # add features to graph nodes\n",
    "    nx.set_node_attributes(G, dict_contig_list_coverage_gc_kmer, \"x\")\n",
    "\n",
    "    # remove all nodes < 100 bp\n",
    "    for node in list(G.nodes):\n",
    "        # print(node, \"neighbors:\", list(G.neighbors(node)), \"len:\", len(list(G.neighbors(node))))\n",
    "        if dict_contig_length[node] < 100:\n",
    "            if len(list(G.neighbors(node))):\n",
    "                # print(\"connecting neighbors of node\", node)\n",
    "                neighbors = list(G.neighbors(node))\n",
    "                all_new_edges = list(itertools.combinations(neighbors, 2))\n",
    "                for edge in all_new_edges:\n",
    "                    G.add_edge(edge[0], edge[1])\n",
    "            # print(\"removing node\", node)\n",
    "            G.remove_node(node)\n",
    "\n",
    "\n",
    "    # add empty labels to graph nodes\n",
    "    dict_contig_label = {}\n",
    "    for contig_ in dict_contig_coverage:\n",
    "        dict_contig_label[contig_] = [0, 0]\n",
    "\n",
    "    nx.set_node_attributes(G, dict_contig_label, \"y\")\n",
    "\n",
    "    # generate spektral graph\n",
    "    the_graph = Networkx_to_Spektral(G)\n",
    "\n",
    "    the_graph.apply(GCNFilter())\n",
    "\n",
    "    loader = SingleLoader(the_graph)\n",
    "\n",
    "    preds = model.predict(loader.load(), steps=loader.steps_per_epoch)\n",
    "\n",
    "    # prediction to df\n",
    "    list_of_lists_with_prediction = []\n",
    "    for index, contig in enumerate(G.nodes):\n",
    "        contig_name = contig\n",
    "        plasmid_probability = preds[index][0]\n",
    "        chromosome_probability = preds[index][1]\n",
    "        # label\n",
    "        if list(np.around(preds[index])) == [0, 0]:\n",
    "            label = \"not_labelled\"\n",
    "        elif list(np.around(preds[index])) == [0, 1]:\n",
    "            label = \"Chromosome\"\n",
    "        elif list(np.around(preds[index])) == [1, 0]:\n",
    "            label = \"Plasmid\"\n",
    "        elif list(np.around(preds[index])) == [1, 1]:\n",
    "            label = \"Ambiguous\"\n",
    "        list_of_lists_with_prediction.append(\n",
    "            [contig_name, plasmid_probability, chromosome_probability, label]\n",
    "        )\n",
    "\n",
    "    prediction_df = pd.DataFrame(\n",
    "        list_of_lists_with_prediction,\n",
    "        columns=[\n",
    "            \"contig_name\",\n",
    "            \"plasmid_probability\",\n",
    "            \"chromosome_probability\",\n",
    "            \"predicted_label\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    prediction_df.to_csv(\n",
    "        \"../results/predictions/\" + isolate + \"_prediction_\" + species + \"_model.csv\", index=False\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
